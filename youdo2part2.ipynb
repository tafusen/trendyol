{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41cbde89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will update the previous part 1 code to part 2 requirements. Most of the code is same\n",
    "# Thank you for extending time. I have exams and projects to deliver this week. Thank you for understanding\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b89d1494",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://files.grouplens.org/datasets/movielens/ml-100k/u.data', delimiter=r'\\t',\n",
    "                 names=['user_id', 'item_id', 'rating', 'timestamp'], engine='python') \n",
    " \n",
    "r = df.pivot(index='user_id', columns='item_id', values='rating').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bde87d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 1682)\n"
     ]
    }
   ],
   "source": [
    "#printing r size\n",
    "print(r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f00c4dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving locations of non na cells\n",
    "row,col = np.where(~np.isnan(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2491def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   21,    81,   142,   179,   197,   293,   375,   543,   667,\n",
       "         693,   789,   814,   949,   970,   977,  1071,  1273,  1448,\n",
       "        1514,  1726,  1758,  1782,  1789,  1842,  1863,  2160,  2259,\n",
       "        2296,  2498,  2546,  2556,  2633,  2706,  2763,  3051,  3238,\n",
       "        3294,  3375,  3507,  3570,  3758,  4124,  4153,  4222,  4300,\n",
       "        4403,  4508,  4828,  4876,  4923,  4988,  5188,  5260,  5280,\n",
       "        5287,  5348,  5382,  5550,  5595,  5608,  5961,  5981,  5999,\n",
       "        6067,  6265,  6348,  6391,  6431,  6861,  7035,  7115,  7203,\n",
       "        7335,  7607,  7626,  7663,  7683,  7881,  7974,  8074,  8135,\n",
       "        8149,  8314,  8376,  8415,  8526,  8551,  8563,  8679,  8714,\n",
       "        8735,  8838,  8885,  9017,  9045,  9246,  9265,  9438,  9540,\n",
       "        9542,  9557,  9642,  9672,  9731,  9798,  9858, 10057, 10061,\n",
       "       10112, 10181, 10251, 10395, 10467, 10643, 10743, 10782, 10900,\n",
       "       10943, 11022, 11063, 11127, 11138, 11173, 11319, 11453, 11480,\n",
       "       11481, 11487, 11614, 11781, 11916, 11968, 12246, 12286, 12290,\n",
       "       12363, 12446, 12584, 12647, 12876, 13010, 13063, 13143, 13162,\n",
       "       13234, 13350, 13352, 13419, 13446, 13497, 13562, 13569, 13828,\n",
       "       13833, 14009, 14025, 14083, 14183, 14272, 14564, 14743, 14787,\n",
       "       14837, 14853, 14855, 14963, 15328, 15407, 15556, 15579, 15634,\n",
       "       15652, 15701, 15725, 15735, 15764, 15841, 15866, 15918, 15943,\n",
       "       15987, 16127, 16155, 16163, 16165, 16231, 16348, 16360, 16386,\n",
       "       16394, 16413, 16731, 16830, 16908, 16952, 17062, 17265, 17276,\n",
       "       17310, 17326, 17332, 17529, 17552, 17555, 17589, 17624, 17639,\n",
       "       17918, 17953, 17975, 18127, 18138, 18189, 18233, 18395, 18481,\n",
       "       18560, 18721, 18820, 18927, 19266, 19611, 19728, 19846, 19944,\n",
       "       19967, 19998, 20047, 20108, 20144, 20338, 20466, 20512, 20539,\n",
       "       20635, 20696, 20731, 20759, 21060, 21077, 21090, 21179, 21208,\n",
       "       21336, 21352, 21423, 21478, 21816, 21833, 21860, 21875, 22099,\n",
       "       22183, 22459, 22683, 22729, 22843, 22911, 22983, 23201, 23390,\n",
       "       23711, 23744, 23838, 23840, 23845, 23913, 23968, 24022, 24066,\n",
       "       24197, 24360, 24559, 24691, 24890, 24903, 25000, 25022, 25030,\n",
       "       25248, 25251, 25273, 25286, 25660, 25663, 25686, 25788, 25822,\n",
       "       25858, 25862, 26226, 26364, 26468, 26555, 26629, 26663, 26744,\n",
       "       26754, 26775, 26856, 26921, 26928, 26980, 26984, 27093, 27129,\n",
       "       27193, 27300, 27375, 27545, 27625, 27647, 27707, 27884, 27958,\n",
       "       28127, 28242, 28698, 28920, 29169, 29237, 29318, 29542, 29544,\n",
       "       29611, 29707, 29762, 29771, 29804, 29913, 29988, 30039, 30050,\n",
       "       30151, 30365, 30368, 30413, 30506, 30547, 30601, 30845, 31210,\n",
       "       31238, 31527, 31633, 31667, 31851, 31867, 31914, 32053, 32079,\n",
       "       32135, 32187, 32244, 32287, 32408, 32626, 32704, 32737, 33013,\n",
       "       33207, 33233, 33294, 33382, 33416, 33468, 33491, 33601, 33604,\n",
       "       33692, 33695, 34031, 34171, 34322, 34364, 34381, 34423, 34474,\n",
       "       34659, 34664, 34677, 34714, 35043, 35307, 35315, 35694, 35757,\n",
       "       35759, 35823, 35882, 36124, 36155, 36218, 36298, 36314, 36433,\n",
       "       36587, 36611, 36647, 36677, 36694, 36851, 36964, 37114, 37406,\n",
       "       37577, 37613, 37693, 37976, 38155, 38210, 38288, 38307, 38345,\n",
       "       38366, 38387, 38500, 38511, 38676, 38758, 38809, 39372, 39610,\n",
       "       39681, 39732, 39831, 39868, 39994, 40060, 40067, 40178, 40250,\n",
       "       40273, 40357, 40397, 40420, 40422, 40496, 40518, 40526, 40600,\n",
       "       40839, 41014, 41058, 41224, 41294, 41343, 41661, 41938, 42143,\n",
       "       42233, 42288, 42321, 42404, 42467, 42591, 42678, 42723, 42757,\n",
       "       42888, 43039, 43070, 43111, 43146, 43234, 43346, 43348, 43356,\n",
       "       43376, 43386, 43433, 43514, 43523, 43705, 43740, 43759, 43806,\n",
       "       43983, 44006, 44122, 44331, 44415, 44418, 44549, 44560, 44636,\n",
       "       44770, 45005, 45057, 45562, 45608, 45660, 46093, 46425, 46434,\n",
       "       46445, 46740, 46802, 46858, 46907, 46952, 46971, 47033, 47049,\n",
       "       47070, 47088, 47217, 47246, 47256, 47287, 47403, 47640, 47761,\n",
       "       47805, 47906, 47944, 48002, 48047, 48463, 48656, 48674, 48718,\n",
       "       48797, 48821, 48890, 48924, 49092, 49096, 49222, 49225, 49235,\n",
       "       49435, 49524, 49560, 49875, 49999, 50124, 50268, 50382, 50451,\n",
       "       50485, 50551, 50705, 50821, 50870, 51054, 51308, 51366, 51416,\n",
       "       51921, 51949, 52116, 52199, 52234, 52263, 52332, 52382, 52478,\n",
       "       52489, 52490, 52685, 52723, 53014, 53066, 53149, 53229, 53233,\n",
       "       53584, 53615, 53631, 53682, 53699, 54200, 54246, 54360, 54475,\n",
       "       54553, 54562, 54601, 54911, 55001, 55074, 55123, 55209, 55219,\n",
       "       55386, 55493, 55589, 55855, 55880, 55904, 55925, 55968, 55994,\n",
       "       56017, 56018, 56462, 56477, 56516, 56520, 56888, 56973, 56990,\n",
       "       57035, 57125, 57151, 57297, 57381, 57451, 57472, 57504, 57692,\n",
       "       57716, 57735, 57865, 57887, 58041, 58067, 58320, 58343, 58776,\n",
       "       58847, 58864, 58972, 58985, 59165, 59168, 59192, 59696, 59699,\n",
       "       59711, 60031, 60131, 60144, 60163, 60211, 60346, 60443, 60504,\n",
       "       60535, 60721, 60795, 60831, 60924, 61085, 61270, 61533, 61848,\n",
       "       62023, 62511, 62539, 62596, 62601, 62652, 62692, 62707, 62708,\n",
       "       62831, 62839, 63193, 63512, 63662, 63784, 63875, 63916, 63979,\n",
       "       64040, 64459, 64727, 64892, 65018, 65242, 65294, 65313, 65321,\n",
       "       65413, 65464, 65493, 65635, 65647, 65746, 65883, 65988, 66197,\n",
       "       66277, 66387, 66727, 66733, 66838, 66877, 67156, 67188, 67290,\n",
       "       67340, 67431, 67490, 67500, 67644, 67688, 67706, 67906, 68106,\n",
       "       68187, 68197, 68368, 68552, 68672, 68905, 68960, 69159, 69423,\n",
       "       69725, 69797, 69864, 69868, 69938, 69964, 70012, 70033, 70036,\n",
       "       70178, 70247, 70277, 70366, 70404, 70443, 70447, 70478, 70495,\n",
       "       70547, 70669, 70681, 70690, 70717, 71409, 71433, 71485, 71616,\n",
       "       71706, 71721, 71846, 71853, 71864, 71914, 72056, 72118, 72180,\n",
       "       72322, 72328, 72378, 72500, 72528, 72642, 72647, 72823, 72876,\n",
       "       73088, 73105, 73118, 73447, 73756, 73873, 73938, 73977, 74019,\n",
       "       74255, 74291, 74392, 74411, 74538, 74663, 74929, 75134, 75320,\n",
       "       75435, 75523, 75648, 76112, 76223, 76241, 76337, 76566, 76844,\n",
       "       76881, 76984, 77031, 77146, 77266, 77282, 77481, 77506, 77519,\n",
       "       77524, 77544, 77547, 77653, 77815, 77901, 78109, 78177, 78235,\n",
       "       78250, 78310, 78647, 78822, 79007, 79034, 79517, 79749, 79765,\n",
       "       79807, 79867, 80387, 80393, 80490, 80680, 80765, 81146, 81195,\n",
       "       81294, 81358, 81368, 81405, 81646, 81881, 81964, 82224, 82280,\n",
       "       82326, 82403, 82535, 82601, 82719, 82725, 82786, 82994, 83068,\n",
       "       83411, 83452, 83466, 83670, 83824, 83907, 83909, 83997, 84315,\n",
       "       84517, 84547, 84591, 84617, 84649, 84842, 85342, 85343, 85598,\n",
       "       85689, 85801, 85929, 85982, 86076, 86134, 86162, 86213, 86272,\n",
       "       86335, 86732, 86846, 86855, 86941, 87043, 87086, 87158, 87285,\n",
       "       87286, 87430, 87450, 87673, 87743, 87861, 87862, 87870, 87875,\n",
       "       88273, 88325, 88376, 88589, 88827, 88914, 89021, 89050, 89210,\n",
       "       89323, 89404, 89479, 89569, 89646, 89682, 89685, 89706, 89775,\n",
       "       89897, 90036, 90088, 90128, 90153, 90249, 90394, 90526, 90531,\n",
       "       90565, 90765, 90814, 90889, 90942, 90948, 91131, 91132, 91152,\n",
       "       91312, 91395, 91410, 91753, 91851, 91867, 91991, 92064, 92101,\n",
       "       92277, 92334, 92547, 92559, 92693, 93036, 93050, 93182, 93335,\n",
       "       93406, 93426, 93438, 93524, 93579, 93602, 93818, 93852, 93866,\n",
       "       94100, 94180, 94329, 94413, 94508, 94544, 94589, 94674, 94731,\n",
       "       94842, 94920, 94970, 94972, 95009, 95023, 95177, 95346, 95396,\n",
       "       95420, 95604, 95670, 95771, 95778, 95991, 96215, 96248, 96478,\n",
       "       96503, 96566, 96683, 96801, 97071, 97333, 97364, 97475, 97500,\n",
       "       97502, 97569, 97872, 97975, 98054, 98182, 98288, 98495, 98680,\n",
       "       98694, 98756, 98841, 99026, 99385, 99487, 99540, 99592, 99629,\n",
       "       99991])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k= np.random.choice(np.arange(100000), 1000, replace=False)\n",
    "k.sort()\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fb4dc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "testrow= row[k]\n",
    "testcol=col[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78e9db1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_copy = r.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc9e91fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(testrow.size):\n",
    "    r_copy[testrow[index]][testcol[index]] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79866f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "row2, col2 = np.where(~np.isnan(r_copy))\n",
    "son=r_copy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd4b73ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I created lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d39a4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamda:  0.0 Loss:  41322.3886088598\n",
      "Lamda:  0.01 Loss:  41368.202610085165\n",
      "Lamda:  0.02 Loss:  41432.547927412495\n",
      "Lamda:  0.03 Loss:  41437.1742673999\n",
      "Lamda:  0.04 Loss:  41480.622562420554\n",
      "Lamda:  0.05 Loss:  41494.84605676925\n",
      "Lamda:  0.06 Loss:  41583.155543078894\n",
      "Lamda:  0.07 Loss:  41595.94950684946\n",
      "Lamda:  0.08 Loss:  41615.86908981783\n",
      "Lamda:  0.09 Loss:  41656.00809819589\n",
      "Lamda:  0.1 Loss:  41661.22926721094\n",
      "Lamda:  0.11 Loss:  41732.35086210293\n",
      "Lamda:  0.12 Loss:  41752.79253052781\n",
      "Lamda:  0.13 Loss:  41814.19441338\n",
      "Lamda:  0.14 Loss:  41826.21125621404\n",
      "Lamda:  0.15 Loss:  41872.19509520432\n",
      "Lamda:  0.16 Loss:  41899.046043250324\n",
      "Lamda:  0.17 Loss:  41900.77647919816\n",
      "Lamda:  0.18 Loss:  41973.3929070008\n",
      "Lamda:  0.19 Loss:  42001.28404961011\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.001  #learning rate\n",
    "beta_user = np.random.rand(r.shape[0]) #random vector value beta for 963 users\n",
    "beta_item = np.random.rand(r.shape[1]) #random vector value beta for 1682 users\n",
    "\n",
    "lamda_array = np.arange(0,0.20, 0.01)\n",
    "\n",
    "for lamda in lamda_array:\n",
    "    # I am resetting matrix for each lamda values so they will be calculated independently\n",
    "    # I calculated and without resetting it gets similar calculations as well but I think this way resetting everything\n",
    "    # for each lamdas is cleaner solution for the homework\n",
    "    loss_pre = 1000000000\n",
    "    row,col = np.where(~np.isnan(r))\n",
    "    rancho= np.random.choice(np.arange(100000), 1000, replace=False)\n",
    "    rancho.sort()\n",
    "    testrow= row[rancho]\n",
    "    testcol=col[rancho]\n",
    "    r_copy = r.copy()\n",
    "    row,col = np.where(~np.isnan(r))\n",
    "    for index in range(testrow.size):\n",
    "        r_copy[testrow[index]][testcol[index]] = np.nan\n",
    "    row2, col2 = np.where(~np.isnan(r_copy))\n",
    "    son=r_copy.copy()\n",
    "    beta_user = np.random.rand(r.shape[0]) #random vector value beta for 963 users\n",
    "    beta_item = np.random.rand(r.shape[1])\n",
    "        \n",
    "    while(True):\n",
    "\n",
    "        loss = 0\n",
    "        for index in range(row2.size):\n",
    "            r_pred = beta_user[row2[index]] + beta_item[col2[index]]\n",
    "            r_real = r_copy[row2[index]][col2[index]]\n",
    "            loss +=  ((r_real - r_pred) ** 2) /2\n",
    "        #loss for lamdas as well added for square sum\n",
    "        a=0\n",
    "        for i in range (len(beta_item)):\n",
    "            a+=beta_item[i] **2\n",
    "        for i in range (len(beta_user)):\n",
    "            a+=beta_user[i]**2\n",
    "        #sums of them * lamda divided by 2\n",
    "        a=a*lamda /2\n",
    "        #add sums to loss\n",
    "        loss+=a\n",
    "\n",
    "\n",
    "        #drop loop if loss gets bigger or moves really small\n",
    "        if(loss > loss_pre or loss_pre-loss < 0.2):\n",
    "            break\n",
    "        loss_pre = loss\n",
    "\n",
    "\n",
    "        g_beta_user = np.zeros(r.shape[0])  #gradient calc. for user\n",
    "        g_beta_item = np.zeros(r.shape[1])  #gradient calc. for item\n",
    "\n",
    "\n",
    "        for index in range(row2.size):\n",
    "            row = row2[index]\n",
    "            col = col2[index]\n",
    "            #update is same for user and item because derivative is same \n",
    "            update = beta_user[row] + beta_item[col] - r_copy[row][col]\n",
    "            g_beta_user[row] += update\n",
    "            g_beta_item[col] += update\n",
    "        #adding the lamdas for betas\n",
    "        for i in range(len(beta_item)):\n",
    "            g_beta_item[i]+=lamda*beta_item[i]\n",
    "        for i in range(len(beta_user)):\n",
    "            g_beta_user[i]+=lamda*beta_item[i]\n",
    "\n",
    "\n",
    "        #update final values\n",
    "        beta_user = beta_user - g_beta_user * alpha\n",
    "        beta_item = beta_item - g_beta_item * alpha\n",
    "    # now lets calculate loss functionos for each lamdas\n",
    "    if(loss_pre<loss):\n",
    "        loss=loss_pre\n",
    "    print(\"Lamda: \",lamda,\"Loss: \",loss)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b78b65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
